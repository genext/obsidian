---
title: NLP (Natural Language Processing)
tags:
  - python
  - nlp
  - natural-language-processing
  - text-vectorization
  - machine-learning
---
# NLP (Natural Language Processing)

## Text Vectorization or Encoding
Text vectorization is the process of converting text data into numerical representations that machine learning models can process. Here are the basic approaches:
### Basic Text Vectorization Approaches
#### One-Hot Encoding
Represents each word as a binary vector where only one element is 1 (hot) and all others are 0. Each word gets a unique position in the vector.
#### Byte-Pair Encoding (BPE)
A subword tokenization method that iteratively merges the most frequent pairs of characters or character sequences to build a vocabulary of subword units.
#### Bag of Words (BOW)
Represents text as an unordered collection of words, disregarding grammar and word order but keeping multiplicity. Each document is represented by word frequency counts.
#### N-Grams
Contiguous sequences of N items (words or characters) from a given text sample. Common types include:
- Unigrams (1-gram): individual words
- Bigrams (2-gram): pairs of consecutive words
- Trigrams (3-gram): sequences of three consecutive words
#### Term Frequency-Inverse Document Frequency (TF-IDF)
A numerical statistic that reflects how important a word is to a document in a collection. It combines:
- **Term Frequency (TF)**: How often a word appears in a document
- **Inverse Document Frequency (IDF)**: How rare or common a word is across all documents
#### N-Gram Language Model with NLTK
Statistical language models that predict the probability of a word based on the previous N-1 words, implemented using the Natural Language Toolkit (NLTK) library.

---

## Related Topics
- [[Machine Learning]]
- [[Text Processing]]
- [[Feature Engineering]]
