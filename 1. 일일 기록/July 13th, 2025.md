---
title: "July 13th, 2025"
created: 2025-07-13 08:00:50
updated: 2025-08-04 21:14:46
---
  * 오늘 할 일
    * claude code로 지난 금요일에 작성한 pdf2markdown.py를 수정하고 프로그램 분석
    * 일주일 회고
  * 명경지수 -> 명징한 생각
  * 오늘 일 중 꼭 기억할 것은?
  * 내가 한 일 중 중요하고도 잘 했던 일은?
  * 08:00 뭔가 꼬였다. claude code를 설치했지만 한동안 사용하지 않다가 이번 skt 프로젝트 pdf to markdown 건으로 만든 프로그램을 검사시키려고 실행하는데 먼저 update하라고 했다. update 했는데도 자꾸 update하라고 나온다.
    * 할 수 없이 npm 명령어로 uninstall했는데 "claude --version"이 실행된다. 그래서 sudo로 다시 똑같은 명령어를 실행하니까 그제서야 없어졌다. 전에 sudo로 실행했나?
    * 다시 install했는데 디렉토리나 환경 파일은 만들어졌는데 claude를 못 찾는다.
    * 검색해 보니 https://github.com/anthropics/claude-code/issues/1682에 다른 방식으로 설치를 시도할 수 있다. 그런데 설치 디렉토리는 PATH에 없는 ~.local/bin/
    * ~/.zshrc의 PATH에 위 디렉토리를 추가해도 계정 홈에서만 claude를 찾고 그 아래 디렉토리에서는 못 찾는다. 이상한데?
  * 09:17 cig 3
  * 소스 분석 중 의문점
    * pdf2markdown.py 초기본
      * ```python
import os
import sys
from pathlib import Path
from typing import Dict, List, Any
import pymupdf4llm
import pymupdf  # For additional PDF operations if needed


def load_pdf(directory: str, file_name: str) -> str:
    """
    Load a PDF file and return its content as text using pymupdf4llm.
    
    Args:
        directory: Directory path where the PDF file is located
        file_name: Name of the PDF file
        
    Returns:
        Extracted text content from PDF
        
    Raises:
        FileNotFoundError: If file doesn't exist
        ValueError: If file is not a PDF
    """
    # Create full file path
    file_path = Path(directory) / file_name
    
    # Check if directory exists
    if not Path(directory).exists():
        raise FileNotFoundError(f"Directory does not exist: {directory}")
    
    # Check if file exists
    if not file_path.exists():
        raise FileNotFoundError(f"File does not exist: {file_path}")
    
    # Check if file is a PDF
    if file_path.suffix.lower() != '.pdf':
        raise ValueError(f"File is not a PDF: {file_name}")
    
    # Load PDF using pymupdf4llm
    try:
        # pymupdf4llm.to_markdown() extracts text with markdown formatting
        pdf_text = pymupdf4llm.to_markdown(str(file_path))
        return pdf_text
    except Exception as e:
        raise Exception(f"Error loading PDF: {str(e)}")


def process_pdf(pdf_data: str) -> Dict[str, Any]:
    """
    Process the PDF data and analyze its structure.
    
    Args:
        pdf_data: Raw text data from PDF
        
    Returns:
        Dictionary containing processed data with structure analysis
    """
    # Split content by common page markers if present
    # pymupdf4llm usually preserves some structure
    pages = []
    
    # Try to split by page breaks if they exist in the markdown
    # This is a simple approach - pymupdf4llm might include page info
    content_parts = pdf_data.split('\n\n')
    
    # Group content into logical sections
    current_page = []
    page_num = 1
    
    for part in content_parts:
        current_page.append(part)
        
        # Simple heuristic: if we have accumulated enough content, consider it a page
        # This is simplified since pymupdf4llm doesn't always preserve page boundaries
        if len('\n'.join(current_page)) > 3000:  # Approximate page size
            pages.append({
                'page_number': page_num,
                'content': '\n\n'.join(current_page),
                'sections': extract_sections('\n\n'.join(current_page))
            })
            current_page = []
            page_num += 1
    
    # Don't forget the last page
    if current_page:
        pages.append({
            'page_number': page_num,
            'content': '\n\n'.join(current_page),
            'sections': extract_sections('\n\n'.join(current_page))
        })
    
    # If no pages were created, treat entire content as one page
    if not pages:
        pages.append({
            'page_number': 1,
            'content': pdf_data,
            'sections': extract_sections(pdf_data)
        })
    
    # Analyze overall structure
    processed_data = {
        'total_pages': len(pages),
        'pages': pages,
        'metadata': {
            'total_characters': len(pdf_data),
            'total_lines': pdf_data.count('\n'),
            'has_headers': any('#' in page['content'] for page in pages),
            'has_lists': any(['*' in page['content'] or '-' in page['content'] 
                             for page in pages]),
            'has_tables': any('|' in page['content'] for page in pages)
        }
    }
    
    return processed_data


def extract_sections(content: str) -> List[Dict[str, str]]:
    """
    Extract sections from content based on markdown headers.
    
    Args:
        content: Page content
        
    Returns:
        List of sections with titles and content
    """
    sections = []
    lines = content.split('\n')
    current_section = {'title': 'Introduction', 'content': []}
    
    for line in lines:
        if line.strip().startswith('#'):
            # Save previous section if it has content
            if current_section['content']:
                current_section['content'] = '\n'.join(current_section['content'])
                sections.append(current_section)
            
            # Start new section
            current_section = {
                'title': line.strip().lstrip('#').strip(),
                'content': []
            }
        else:
            current_section['content'].append(line)
    
    # Don't forget the last section
    if current_section['content']:
        current_section['content'] = '\n'.join(current_section['content'])
        sections.append(current_section)
    
    return sections


def markdown_save(preprocessed_data: Dict[str, Any], output_path: str = None) -> str:
    """
    Convert preprocessed PDF data to markdown format and save it.
    
    Args:
        preprocessed_data: Processed PDF data from process_pdf()
        output_path: Optional output file path. If not provided, returns markdown string
        
    Returns:
        Markdown formatted string
    """
    markdown_lines = []
    
    # Add document metadata
    markdown_lines.append("# Document Analysis\n")
    markdown_lines.append(f"**Total Pages**: {preprocessed_data['total_pages']}")
    markdown_lines.append(f"**Total Characters**: {preprocessed_data['metadata']['total_characters']}")
    markdown_lines.append(f"**Total Lines**: {preprocessed_data['metadata']['total_lines']}\n")
    
    # Add structure analysis
    markdown_lines.append("## Document Structure\n")
    if preprocessed_data['metadata']['has_headers']:
        markdown_lines.append("- ✓ Contains headers")
    if preprocessed_data['metadata']['has_lists']:
        markdown_lines.append("- ✓ Contains lists")
    if preprocessed_data['metadata']['has_tables']:
        markdown_lines.append("- ✓ Contains tables")
    markdown_lines.append("\n---\n")
    
    # Add content by pages
    markdown_lines.append("## Content\n")
    
    for page in preprocessed_data['pages']:
        markdown_lines.append(f"### Page {page['page_number']}\n")
        
        # Add sections if found
        if page['sections']:
            for section in page['sections']:
                if section['title'] != 'Introduction' or section['content'].strip():
                    markdown_lines.append(f"#### {section['title']}\n")
                    markdown_lines.append(section['content'])
                    markdown_lines.append("")
        else:
            # If no sections, just add the content
            markdown_lines.append(page['content'])
        
        markdown_lines.append("\n---\n")
    
    # Join all lines
    markdown_content = '\n'.join(markdown_lines)
    
    # Save to file if output path is provided
    if output_path:
        output_file = Path(output_path)
        output_file.parent.mkdir(parents=True, exist_ok=True)
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(markdown_content)
        print(f"Markdown saved to: {output_path}")
    
    return markdown_content


def main():
    """
    Main function to test the PDF to Markdown conversion.
    """
    # Example usage
    try:
        # Test parameters - modify these for your use case
        directory = "."  # Current directory
        file_name = "example.pdf"  # Change to your PDF file name
        output_file = "output.md"  # Output markdown file
        
        print(f"Loading PDF: {file_name} from {directory}")
        pdf_data = load_pdf(directory, file_name)
        print("✓ PDF loaded successfully")
        
        print("\nProcessing PDF data...")
        processed_data = process_pdf(pdf_data)
        print(f"✓ Found {processed_data['total_pages']} pages")
        print(f"✓ Document contains {processed_data['metadata']['total_characters']} characters")
        
        print("\nConverting to Markdown...")
        markdown_content = markdown_save(processed_data, output_file)
        print(f"✓ Markdown saved to {output_file}")
        
        # Also print first 500 characters as preview
        print("\nMarkdown preview:")
        print("-" * 50)
        print(markdown_content[:500] + "..." if len(markdown_content) > 500 else markdown_content)
        
    except FileNotFoundError as e:
        print(f"Error: {e}")
        print("\nPlease make sure to:")
        print("1. Place your PDF file in the specified directory")
        print("2. Update the 'directory' and 'file_name' variables in main()")
    except ValueError as e:
        print(f"Error: {e}")
    except Exception as e:
        print(f"Unexpected error: {e}")
        print("\nMake sure you have installed required packages:")
        print("pip install pymupdf4llm")


if *name* == "*main*":
    main()```
    * pymupdf4llm.to_markdown()은 pdf 문서에 있는 모든 표식을 과연 다 포함하나?
    * 크기를 확인할 때 왜 굳이 '\n'을 사이에 넣고 추가할까? 최종 크기만 더 커지는데..
    * common page marker? page breaker in markdown?
      * 주석에서 다른 단어를 썼는데 결국 page marker라고 부르든 page breaker라고 부르든 pdf_data를 '\n\n'로 나눠서 content_part의 각 요소에 쪼개 넣겠다는 뜻.
    * pymupdf4llm이 페이지 경계를 항상 꼭 구분해서 나누지 않는다는 말이 반복된다.
    * 이 소스를 찬찬히 살펴봤는데 문제가 꽤 있다. 일단 extract_section에서 '#'를 lstrip으로 뺐으면서 나중에 모든 결과를 모아서 집계할 때는 '#'을 찾아서 세려고 한다. 아무튼 cluade라도 일반 AI가 코딩하는 것을 그대로 쓰면 안 되고 로직을 잘 살펴봐야 한다. "claude code"를 쓰는 것이 더 낫다.
  * 10:45 cig 4
  * 11:17 cig 5
  * 소스 분석 중 의문점
    * pdf2markdown.py 초기본에 로그와 디버깅 정보를 추가한 것
      * ```python
import os
import sys
import json
import logging
from pathlib import Path
from typing import Dict, List, Any, Optional
import pymupdf4llm
import pymupdf  # For additional PDF operations
from datetime import datetime


def setup_logging(log_dir: str = "logs") -> logging.Logger:
    """
    Set up detailed logging configuration.
    
    Args:
        log_dir: Directory to store log files
        
    Returns:
        Configured logger instance
    """
    # Create logs directory if it doesn't exist
    Path(log_dir).mkdir(exist_ok=True)
    
    # Create timestamp for log file
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_file = Path(log_dir) / f"pdf_conversion_{timestamp}.log"
    
    # Configure logging
    logging.basicConfig(
        level=logging.DEBUG,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_file, encoding='utf-8'),
            logging.StreamHandler(sys.stdout)
        ]
    )
    
    logger = logging.getLogger('PDFConverter')
    logger.info(f"Logging initialized. Log file: {log_file}")
    
    return logger


def load_pdf(directory: str, file_name: str, logger: logging.Logger) -> Dict[str, Any]:
    """
    Load a PDF file with detailed analysis and return its content.
    
    Args:
        directory: Directory path where the PDF file is located
        file_name: Name of the PDF file
        logger: Logger instance
        
    Returns:
        Dictionary containing extracted content and metadata
        
    Raises:
        FileNotFoundError: If file doesn't exist
        ValueError: If file is not a PDF
    """
    logger.info(f"Starting PDF load process for: {file_name} in {directory}")
    
    # Create full file path
    file_path = Path(directory) / file_name
    
    # Check if directory exists
    if not Path(directory).exists():
        logger.error(f"Directory does not exist: {directory}")
        raise FileNotFoundError(f"Directory does not exist: {directory}")
    
    # Check if file exists
    if not file_path.exists():
        logger.error(f"File does not exist: {file_path}")
        raise FileNotFoundError(f"File does not exist: {file_path}")
    
    # Check if file is a PDF
    if file_path.suffix.lower() != '.pdf':
        logger.error(f"File is not a PDF: {file_name}")
        raise ValueError(f"File is not a PDF: {file_name}")
    
    # Get file size
    file_size = file_path.stat().st_size
    logger.info(f"PDF file size: {file_size:,} bytes")
    
    result = {}
    
    try:
        # First, analyze PDF structure using PyMuPDF
        logger.info("Analyzing PDF structure with PyMuPDF...")
        doc = pymupdf.open(str(file_path))
        
        # Collect PDF metadata
        metadata = doc.metadata
        logger.info(f"PDF Metadata: {json.dumps(metadata, indent=2)}")
        
        # Analyze each page
        page_analysis = []
        for page_num, page in enumerate(doc, 1):
            logger.debug(f"Analyzing page {page_num}/{doc.page_count}")
            
            page_info = {
                'page_number': page_num,
                'width': page.rect.width,
                'height': page.rect.height,
                'rotation': page.rotation,
                'text_length': len(page.get_text()),
                'image_count': len(page.get_images()),
                'link_count': len(page.get_links()),
                'annotation_count': len(page.annots()),
            }
            
            # Check for tables
            tables = page.find_tables()
            page_info['table_count'] = len(tables)
            
            if tables:
                logger.info(f"Page {page_num} contains {len(tables)} table(s)")
                for i, table in enumerate(tables):
                    logger.debug(f"  Table {i+1}: {table.row_count} rows x {table.col_count} columns")
            
            # Get text blocks
            blocks = page.get_text("blocks")
            page_info['block_count'] = len(blocks)
            logger.debug(f"Page {page_num} has {len(blocks)} text blocks")
            
            page_analysis.append(page_info)
        
        result['page_analysis'] = page_analysis
        result['total_pages'] = doc.page_count
        logger.info(f"PDF has {doc.page_count} pages total")
        
        doc.close()
        
        # Now extract content using pymupdf4llm with different methods
        logger.info("Extracting content with pymupdf4llm...")
        
        # Method 1: Default extraction
        logger.info("Method 1: Default pymupdf4llm extraction")
        pdf_text_default = pymupdf4llm.to_markdown(str(file_path))
        result['default_extraction'] = pdf_text_default
        logger.info(f"Default extraction length: {len(pdf_text_default)} characters")
        
        # Method 2: With page chunks
        logger.info("Method 2: Page-by-page extraction")
        pdf_text_pages = pymupdf4llm.to_markdown(
            str(file_path),
            page_chunks=True
        )
        result['page_extraction'] = pdf_text_pages
        logger.info(f"Page extraction returned {len(pdf_text_pages)} pages")
        
        # Method 3: With additional options
        logger.info("Method 3: Extraction with enhanced options")
        pdf_text_enhanced = pymupdf4llm.to_markdown(
            str(file_path),
            write_images=True,
            image_path="extracted_images",
            page_chunks=False,
            margins=(0, 0, 0, 0)  # No margins
        )
        result['enhanced_extraction'] = pdf_text_enhanced
        logger.info(f"Enhanced extraction length: {len(pdf_text_enhanced)} characters")
        
        # Save raw extractions for debugging
        debug_dir = Path("debug_output")
        debug_dir.mkdir(exist_ok=True)
        
        # Save different extraction methods
        with open(debug_dir / f"default_extraction_{timestamp}.md", 'w', encoding='utf-8') as f:
            f.write(result['default_extraction'])
        
        with open(debug_dir / f"enhanced_extraction_{timestamp}.md", 'w', encoding='utf-8') as f:
            f.write(result['enhanced_extraction'])
        
        logger.info(f"Debug files saved to {debug_dir}")
        
        # Log sample of extracted content
        logger.debug("First 500 characters of default extraction:")
        logger.debug(pdf_text_default[:500])
        
        return result
        
    except Exception as e:
        logger.error(f"Error loading PDF: {str(e)}", exc_info=True)
        raise Exception(f"Error loading PDF: {str(e)}")


def process_pdf(pdf_data: Dict[str, Any], logger: logging.Logger) -> Dict[str, Any]:
    """
    Process the PDF data with detailed analysis and debugging.
    
    Args:
        pdf_data: Dictionary containing various extraction results
        logger: Logger instance
        
    Returns:
        Dictionary containing processed data with detailed analysis
    """
    logger.info("Starting PDF processing...")
    
    # Use the best extraction method based on content
    primary_content = pdf_data.get('enhanced_extraction', pdf_data.get('default_extraction', ''))
    
    # Analyze content patterns
    logger.info("Analyzing content patterns...")
    
    # Check for various markdown elements
    header_count = primary_content.count('#')
    table_indicators = primary_content.count('|')
    list_indicators = primary_content.count('*') + primary_content.count('-')
    code_blocks = primary_content.count('```')
    
    logger.info(f"Content analysis:")
    logger.info(f"  - Headers (#): {header_count}")
    logger.info(f"  - Table indicators (|): {table_indicators}")
    logger.info(f"  - List indicators (*, -): {list_indicators}")
    logger.info(f"  - Code blocks (```): {code_blocks}")
    
    # Process pages if available
    if isinstance(pdf_data.get('page_extraction'), list):
        logger.info(f"Processing {len(pdf_data['page_extraction'])} pages from page extraction")
        pages = []
        
        for i, page_content in enumerate(pdf_data['page_extraction'], 1):
            logger.debug(f"Processing page {i}")
            
            # Analyze page content
            page_analysis = analyze_page_content(page_content, i, logger)
            pages.append(page_analysis)
            
            # Log warnings for potential issues
            if page_analysis['issues']:
                logger.warning(f"Page {i} issues: {', '.join(page_analysis['issues'])}")
    else:
        # Fallback: try to split content into pages
        logger.info("No page extraction available, attempting to split content")
        pages = split_content_into_pages(primary_content, logger)
    
    # Identify potential data loss
    logger.info("Checking for potential data loss...")
    
    # Compare with page analysis from PyMuPDF
    if 'page_analysis' in pdf_data:
        for i, page_info in enumerate(pdf_data['page_analysis'], 1):
            if page_info['table_count'] > 0:
                logger.warning(f"Page {i} contains {page_info['table_count']} table(s) - verify extraction")
            if page_info['image_count'] > 0:
                logger.info(f"Page {i} contains {page_info['image_count']} image(s)")
    
    # Build processed data structure
    processed_data = {
        'total_pages': len(pages),
        'pages': pages,
        'metadata': {
            'total_characters': len(primary_content),
            'total_lines': primary_content.count('\n'),
            'has_headers': header_count > 0,
            'has_lists': list_indicators > 0,
            'has_tables': table_indicators > 10,  # More than 10 pipes suggests tables
            'has_code': code_blocks > 0,
            'header_count': header_count,
            'table_indicators': table_indicators,
            'extraction_methods_used': list(pdf_data.keys())
        },
        'extraction_comparison': compare_extractions(pdf_data, logger),
        'potential_issues': identify_issues(pdf_data, pages, logger)
    }
    
    # Save processed data for debugging
    debug_dir = Path("debug_output")
    with open(debug_dir / f"processed_data_{timestamp}.json", 'w', encoding='utf-8') as f:
        json.dump(processed_data, f, indent=2, ensure_ascii=False)
    
    logger.info(f"Processing complete. Found {len(pages)} pages with {len(processed_data['potential_issues'])} potential issues")
    
    return processed_data


def analyze_page_content(content: str, page_num: int, logger: logging.Logger) -> Dict[str, Any]:
    """
    Analyze individual page content for structure and potential issues.
    """
    logger.debug(f"Analyzing content for page {page_num}")
    
    lines = content.split('\n')
    issues = []
    
    # Check for truncated tables
    table_rows = [line for line in lines if '|' in line]
    if table_rows:
        # Check if table rows have consistent column counts
        column_counts = [line.count('|') for line in table_rows]
        if len(set(column_counts)) > 1:
            issues.append("Inconsistent table column counts")
            logger.warning(f"Page {page_num}: Table may be malformed. Column counts: {column_counts}")
    
    # Check for broken formatting
    if content.count('```') % 2 != 0:
        issues.append("Unclosed code block")
    
    # Check for potential missing content
    if len(content.strip()) < 100 and page_num > 1:
        issues.append("Unusually short page content")
    
    # Extract sections
    sections = extract_sections(content)
    
    return {
        'page_number': page_num,
        'content': content,
        'sections': sections,
        'line_count': len(lines),
        'character_count': len(content),
        'table_count': len(table_rows),
        'issues': issues
    }


def split_content_into_pages(content: str, logger: logging.Logger) -> List[Dict[str, Any]]:
    """
    Attempt to split content into logical pages when page extraction isn't available.
    """
    logger.info("Attempting to split content into logical pages")
    
    # Look for page markers
    page_markers = [
        r'Page \d+',
        r'\n\n\n',  # Multiple newlines might indicate page breaks
        r'^\d+$',  # Page numbers on their own line
    ]
    
    # For now, use a simple approach
    # Split by multiple newlines or character count
    pages = []
    chunks = content.split('\n\n\n')
    
    if len(chunks) > 1:
        logger.info(f"Split content into {len(chunks)} chunks by triple newlines")
    else:
        # Fallback: split by character count
        chunk_size = 4000
        chunks = [content[i:i+chunk_size] for i in range(0, len(content), chunk_size)]
        logger.info(f"Split content into {len(chunks)} chunks by character count")
    
    for i, chunk in enumerate(chunks, 1):
        if chunk.strip():
            page_analysis = analyze_page_content(chunk, i, logger)
            pages.append(page_analysis)
    
    return pages


def compare_extractions(pdf_data: Dict[str, Any], logger: logging.Logger) -> Dict[str, Any]:
    """
    Compare different extraction methods to identify differences.
    """
    logger.info("Comparing extraction methods...")
    
    comparison = {}
    
    if 'default_extraction' in pdf_data and 'enhanced_extraction' in pdf_data:
        default_len = len(pdf_data['default_extraction'])
        enhanced_len = len(pdf_data['enhanced_extraction'])
        
        comparison['length_difference'] = enhanced_len - default_len
        comparison['length_ratio'] = enhanced_len / default_len if default_len > 0 else 0
        
        logger.info(f"Extraction length difference: {comparison['length_difference']} characters")
        logger.info(f"Enhanced/Default ratio: {comparison['length_ratio']:.2f}")
        
        # Check what's in enhanced but not in default
        if enhanced_len > default_len * 1.1:  # More than 10% larger
            logger.warning("Enhanced extraction is significantly larger - may contain additional content")
    
    return comparison


def identify_issues(pdf_data: Dict[str, Any], pages: List[Dict[str, Any]], logger: logging.Logger) -> List[str]:
    """
    Identify potential issues in the extraction.
    """
    issues = []
    
    # Check for table issues
    if 'page_analysis' in pdf_data:
        total_tables = sum(p['table_count'] for p in pdf_data['page_analysis'])
        extracted_tables = sum(p.get('table_count', 0) for p in pages)
        
        if total_tables > extracted_tables:
            issue = f"Possible table loss: PDF has {total_tables} tables, extracted {extracted_tables}"
            issues.append(issue)
            logger.warning(issue)
    
    # Check for very short pages
    short_pages = [p['page_number'] for p in pages if p['character_count'] < 100]
    if short_pages:
        issue = f"Very short pages detected: {short_pages}"
        issues.append(issue)
        logger.warning(issue)
    
    # Check for formatting issues
    for page in pages:
        if page.get('issues'):
            issues.extend([f"Page {page['page_number']}: {issue}" for issue in page['issues']])
    
    return issues


def extract_sections(content: str) -> List[Dict[str, str]]:
    """
    Extract sections from content based on markdown headers.
    
    Args:
        content: Page content
        
    Returns:
        List of sections with titles and content
    """
    sections = []
    lines = content.split('\n')
    current_section = {'title': 'Introduction', 'content': [], 'level': 0}
    
    for line in lines:
        if line.strip().startswith('#'):
            # Save previous section if it has content
            if current_section['content']:
                current_section['content'] = '\n'.join(current_section['content'])
                sections.append(current_section)
            
            # Determine header level
            level = len(line.strip()) - len(line.strip().lstrip('#'))
            
            # Start new section
            current_section = {
                'title': line.strip().lstrip('#').strip(),
                'content': [],
                'level': level
            }
        else:
            current_section['content'].append(line)
    
    # Don't forget the last section
    if current_section['content']:
        current_section['content'] = '\n'.join(current_section['content'])
        sections.append(current_section)
    
    return sections


def markdown_save(preprocessed_data: Dict[str, Any], output_path: str = None, logger: logging.Logger = None) -> str:
    """
    Convert preprocessed PDF data to markdown format with detailed debugging.
    
    Args:
        preprocessed_data: Processed PDF data from process_pdf()
        output_path: Optional output file path
        logger: Logger instance
        
    Returns:
        Markdown formatted string
    """
    if logger:
        logger.info("Starting markdown conversion...")
    
    markdown_lines = []
    
    # Add document metadata
    markdown_lines.append("# Document Analysis Report\n")
    markdown_lines.append(f"**Generated**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
    
    # Add extraction summary
    markdown_lines.append("## Extraction Summary\n")
    markdown_lines.append(f"**Total Pages**: {preprocessed_data['total_pages']}")
    markdown_lines.append(f"**Total Characters**: {preprocessed_data['metadata']['total_characters']}")
    markdown_lines.append(f"**Total Lines**: {preprocessed_data['metadata']['total_lines']}")
    markdown_lines.append(f"**Extraction Methods**: {', '.join(preprocessed_data['metadata']['extraction_methods_used'])}\n")
    
    # Add potential issues if any
    if preprocessed_data.get('potential_issues'):
        markdown_lines.append("## ⚠️ Potential Issues\n")
        for issue in preprocessed_data['potential_issues']:
            markdown_lines.append(f"- {issue}")
        markdown_lines.append("")
    
    # Add structure analysis
    markdown_lines.append("## Document Structure\n")
    if preprocessed_data['metadata']['has_headers']:
        markdown_lines.append(f"- ✓ Contains headers ({preprocessed_data['metadata']['header_count']} found)")
    if preprocessed_data['metadata']['has_lists']:
        markdown_lines.append("- ✓ Contains lists")
    if preprocessed_data['metadata']['has_tables']:
        markdown_lines.append(f"- ✓ Contains tables ({preprocessed_data['metadata']['table_indicators']} table indicators)")
    if preprocessed_data['metadata']['has_code']:
        markdown_lines.append("- ✓ Contains code blocks")
    markdown_lines.append("\n---\n")
    
    # Add extraction comparison if available
    if 'extraction_comparison' in preprocessed_data and preprocessed_data['extraction_comparison']:
        markdown_lines.append("## Extraction Method Comparison\n")
        comp = preprocessed_data['extraction_comparison']
        if 'length_difference' in comp:
            markdown_lines.append(f"- Length difference: {comp['length_difference']:+,} characters")
            markdown_lines.append(f"- Size ratio: {comp['length_ratio']:.2f}x\n")
    
    # Add content by pages
    markdown_lines.append("## Document Content\n")
    
    for page in preprocessed_data['pages']:
        markdown_lines.append(f"### Page {page['page_number']}")
        markdown_lines.append(f"*({page['character_count']} characters, {page['line_count']} lines)*\n")
        
        # Add page issues if any
        if page.get('issues'):
            markdown_lines.append("**Page Issues:**")
            for issue in page['issues']:
                markdown_lines.append(f"- ⚠️ {issue}")
            markdown_lines.append("")
        
        # Add sections if found
        if page['sections']:
            for section in page['sections']:
                # Add appropriate number of # based on level
                header_prefix = '#' * (section.get('level', 1) + 3)  # +3 because we're already at ### for page
                markdown_lines.append(f"{header_prefix} {section['title']}\n")
                markdown_lines.append(section['content'])
                markdown_lines.append("")
        else:
            # If no sections, just add the content
            markdown_lines.append(page['content'])
        
        markdown_lines.append("\n---\n")
    
    # Add debug information
    markdown_lines.append("## Debug Information\n")
    markdown_lines.append("- Check `logs/` directory for detailed conversion logs")
    markdown_lines.append("- Check `debug_output/` directory for intermediate extraction files")
    markdown_lines.append("- Review potential issues listed above for content that may need manual verification")
    
    # Join all lines
    markdown_content = '\n'.join(markdown_lines)
    
    # Save to file if output path is provided
    if output_path:
        output_file = Path(output_path)
        output_file.parent.mkdir(parents=True, exist_ok=True)
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(markdown_content)
        if logger:
            logger.info(f"Markdown saved to: {output_path}")
            logger.info(f"Output file size: {output_file.stat().st_size:,} bytes")
    
    return markdown_content


# Global timestamp for consistent file naming
timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")


def main():
    """
    Main function to test the PDF to Markdown conversion with detailed debugging.
    """
    # Set up logging
    logger = setup_logging()
    
    try:
        # Test parameters - modify these for your use case
        directory = "."  # Current directory
        file_name = "example.pdf"  # Change to your PDF file name
        output_file = f"output_{timestamp}.md"  # Output markdown file with timestamp
        
        logger.info("="*60)
        logger.info("Starting PDF to Markdown conversion")
        logger.info(f"Input: {file_name}")
        logger.info(f"Output: {output_file}")
        logger.info("="*60)
        
        # Load PDF with multiple extraction methods
        pdf_data = load_pdf(directory, file_name, logger)
        logger.info("✓ PDF loaded and analyzed successfully")
        
        # Process PDF data
        logger.info("\nProcessing PDF data...")
        processed_data = process_pdf(pdf_data, logger)
        logger.info(f"✓ Found {processed_data['total_pages']} pages")
        logger.info(f"✓ Document contains {processed_data['metadata']['total_characters']} characters")
        
        # Convert to Markdown
        logger.info("\nConverting to Markdown...")
        markdown_content = markdown_save(processed_data, output_file, logger)
        logger.info(f"✓ Markdown saved to {output_file}")
        
        # Summary
        logger.info("\n" + "="*60)
        logger.info("CONVERSION SUMMARY")
        logger.info("="*60)
        logger.info(f"Total pages processed: {processed_data['total_pages']}")
        logger.info(f"Total issues found: {len(processed_data.get('potential_issues', []))}")
        logger.info(f"Output file: {output_file}")
        logger.info(f"Log file: logs/pdf_conversion_{timestamp}.log")
        logger.info(f"Debug files: debug_output/*_{timestamp}.*")
        logger.info("="*60)
        
        if processed_data.get('potential_issues'):
            logger.warning("\nPlease review the potential issues listed in the output file!")
        
    except FileNotFoundError as e:
        logger.error(f"Error: {e}")
        logger.error("\nPlease make sure to:")
        logger.error("1. Place your PDF file in the specified directory")
        logger.error("2. Update the 'directory' and 'file_name' variables in main()")
    except ValueError as e:
        logger.error(f"Error: {e}")
    except Exception as e:
        logger.error(f"Unexpected error: {e}", exc_info=True)
        logger.error("\nMake sure you have installed required packages:")
        logger.error("pip install pymupdf4llm pymupdf")


if *name* == "*main*":
    main()```
    * pdf metadata에서 blocks는 무엇?
  * 15:35 근력 운동 2시간
  * 15:52 cig 8
  * 16:38 cig 9
